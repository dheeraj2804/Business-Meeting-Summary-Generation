{"text": " The following is a conversation with Jan Lekun, his second time on the podcast. He is the chief AI scientist at Metta, formerly Facebook, professor at NYU, touring award winner, one of the seminal figures in the history of machine learning and artificial intelligence, and someone who is brilliant and opinionated in the best kind of way, and so is always fun to talk to. This is Alex Friedman podcast to support it. Please check out our sponsors in the description. And now here's my conversation with Jan Lekun. You co-wrote the article, self-supervised learning, the dark matter of intelligence. Great title, by the way. Will he shine, Ezra? So let me ask, what is self-supervised learning and why is it the dark matter of intelligence? I'll start by the dark matter part. There is obviously a kind of learning that humans and animals are doing that we currently are not reproducing properly with machines or with AI, right? So the most popular approaches to machine learning today are or Pydheims, I should say, are supervised learning and reinforcement learning. And there is too many efficient supervised learning requires many samples for learning anything. And reinforcement learning requires a ridiculously large number of trial and errors to for a system to learn anything. And that's why we don't have self-driving cars. That's a big leap for one to the other. Okay, so to solve difficult problems, you have to have a lot of human annotations for supervised learning to work. And to solve those difficult problems with reinforcement learning, you have to have some way to maybe simulate that problem such that you can do that large scale kind of learning that reinforcement learning requires. Right, so how is it that most teenagers can learn to drive a car in about 20 hours of practice, whereas even with millions of hours of simulated practice, self-driving car can't actually learn to drive itself properly? And so obviously we're missing something, right? And it's quite obvious for a lot of people that the immediate response you get from people is, well, humans use their background knowledge to learn faster. And they're right. Now, how was that background knowledge acquired? And that's the big question. So now you have to ask, how do babies in the first few months of life learn how the world works, mostly by observation, because they can hardly act in the world. And they learn an enormous amount of background knowledge about the world that may be the basis of what we call common sense. This type of learning is not learning a task, it's not being reinforced for anything, it's just observing the world and figuring out how it works. Building world models, learning world models. How do we do this? And how do we reproduce this in machines? So self-supervised learning is one instance or one attempt at trying to reproduce this kind of learning. Okay, so you're looking at just observation. So not even the interacting part of a child. It's just sitting there watching Mom and Dad walk around, pick up stuff, all of that. That's what you mean by background knowledge. Perhaps not even watching Mom and Dad just watching the world go by. Just having eyes open or having eyes closed or the very active opening and closing eyes that the world appears and disappears, all that basic information. And you're saying in order to learn to drive, like the reason humans are able to learn to drive quickly, some faster than others, is because of the background knowledge, they were able to watch cars operate in the world in the many years leading up to it, the physics of basic subjects, all that kind of stuff. That's right. I mean the basic physics of objects, you don't even know, you don't even need to know how car works, because that you can run fairly quickly. I mean the example I use very often is you're driving next to a cliff. And you know in advance, because of your understanding of intuitive physics, that if you turn the wheel to the right, the car will be out to the right, we'll run off the cliff, fall off the cliff, and nothing good will come out of this. But if you are a sort of tabularized reinforcement learning system that doesn't have a model of the world, you have to repeat folding off this cliff thousands of times before you figure out it's a bad idea. And then a few more thousand times before you figure out how to not do it. And then a few more million times before you figure out how to not do it in every situation you ever encounter. So self-supervised learning still has to have some source of truth being told to it by somebody, by some. And so you have to figure out a way without human assistance or without significant amount of human assistance to get that truth from the world. So the", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.72, "text": " The following is a conversation with Jan Lekun, his second time on the podcast.", "tokens": [50364, 440, 3480, 307, 257, 3761, 365, 4956, 441, 916, 409, 11, 702, 1150, 565, 322, 264, 7367, 13, 50600], "temperature": 0.0, "avg_logprob": -0.20144410185761505, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.09085220843553543}, {"id": 1, "seek": 0, "start": 4.72, "end": 9.120000000000001, "text": " He is the chief AI scientist at Metta, formerly Facebook,", "tokens": [50600, 634, 307, 264, 9588, 7318, 12662, 412, 6377, 1328, 11, 34777, 4384, 11, 50820], "temperature": 0.0, "avg_logprob": -0.20144410185761505, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.09085220843553543}, {"id": 2, "seek": 0, "start": 9.120000000000001, "end": 15.6, "text": " professor at NYU, touring award winner, one of the seminal figures in the history", "tokens": [50820, 8304, 412, 42682, 11, 32487, 7130, 8507, 11, 472, 295, 264, 4361, 2071, 9624, 294, 264, 2503, 51144], "temperature": 0.0, "avg_logprob": -0.20144410185761505, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.09085220843553543}, {"id": 3, "seek": 0, "start": 15.6, "end": 21.92, "text": " of machine learning and artificial intelligence, and someone who is brilliant and opinionated", "tokens": [51144, 295, 3479, 2539, 293, 11677, 7599, 11, 293, 1580, 567, 307, 10248, 293, 4800, 770, 51460], "temperature": 0.0, "avg_logprob": -0.20144410185761505, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.09085220843553543}, {"id": 4, "seek": 0, "start": 21.92, "end": 25.36, "text": " in the best kind of way, and so is always fun to talk to.", "tokens": [51460, 294, 264, 1151, 733, 295, 636, 11, 293, 370, 307, 1009, 1019, 281, 751, 281, 13, 51632], "temperature": 0.0, "avg_logprob": -0.20144410185761505, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.09085220843553543}, {"id": 5, "seek": 2536, "start": 25.84, "end": 28.56, "text": " This is Alex Friedman podcast to support it.", "tokens": [50388, 639, 307, 5202, 17605, 1601, 7367, 281, 1406, 309, 13, 50524], "temperature": 0.0, "avg_logprob": -0.22361872488992257, "compression_ratio": 1.625, "no_speech_prob": 0.08414312452077866}, {"id": 6, "seek": 2536, "start": 28.56, "end": 30.72, "text": " Please check out our sponsors in the description.", "tokens": [50524, 2555, 1520, 484, 527, 22593, 294, 264, 3855, 13, 50632], "temperature": 0.0, "avg_logprob": -0.22361872488992257, "compression_ratio": 1.625, "no_speech_prob": 0.08414312452077866}, {"id": 7, "seek": 2536, "start": 30.72, "end": 33.6, "text": " And now here's my conversation with Jan Lekun.", "tokens": [50632, 400, 586, 510, 311, 452, 3761, 365, 4956, 441, 916, 409, 13, 50776], "temperature": 0.0, "avg_logprob": -0.22361872488992257, "compression_ratio": 1.625, "no_speech_prob": 0.08414312452077866}, {"id": 8, "seek": 2536, "start": 33.6, "end": 38.32, "text": " You co-wrote the article, self-supervised learning, the dark matter of intelligence.", "tokens": [50776, 509, 598, 12, 7449, 1370, 264, 7222, 11, 2698, 12, 48172, 24420, 2539, 11, 264, 2877, 1871, 295, 7599, 13, 51012], "temperature": 0.0, "avg_logprob": -0.22361872488992257, "compression_ratio": 1.625, "no_speech_prob": 0.08414312452077866}, {"id": 9, "seek": 2536, "start": 38.32, "end": 39.519999999999996, "text": " Great title, by the way.", "tokens": [51012, 3769, 4876, 11, 538, 264, 636, 13, 51072], "temperature": 0.0, "avg_logprob": -0.22361872488992257, "compression_ratio": 1.625, "no_speech_prob": 0.08414312452077866}, {"id": 10, "seek": 2536, "start": 39.519999999999996, "end": 41.12, "text": " Will he shine, Ezra?", "tokens": [51072, 3099, 415, 12207, 11, 27211, 424, 30, 51152], "temperature": 0.0, "avg_logprob": -0.22361872488992257, "compression_ratio": 1.625, "no_speech_prob": 0.08414312452077866}, {"id": 11, "seek": 2536, "start": 41.12, "end": 46.96, "text": " So let me ask, what is self-supervised learning and why is it the dark matter of intelligence?", "tokens": [51152, 407, 718, 385, 1029, 11, 437, 307, 2698, 12, 48172, 24420, 2539, 293, 983, 307, 309, 264, 2877, 1871, 295, 7599, 30, 51444], "temperature": 0.0, "avg_logprob": -0.22361872488992257, "compression_ratio": 1.625, "no_speech_prob": 0.08414312452077866}, {"id": 12, "seek": 2536, "start": 46.96, "end": 48.8, "text": " I'll start by the dark matter part.", "tokens": [51444, 286, 603, 722, 538, 264, 2877, 1871, 644, 13, 51536], "temperature": 0.0, "avg_logprob": -0.22361872488992257, "compression_ratio": 1.625, "no_speech_prob": 0.08414312452077866}, {"id": 13, "seek": 4880, "start": 49.76, "end": 58.959999999999994, "text": " There is obviously a kind of learning that humans and animals are doing that we currently are not", "tokens": [50412, 821, 307, 2745, 257, 733, 295, 2539, 300, 6255, 293, 4882, 366, 884, 300, 321, 4362, 366, 406, 50872], "temperature": 0.0, "avg_logprob": -0.23898110906761813, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.15858766436576843}, {"id": 14, "seek": 4880, "start": 58.959999999999994, "end": 61.92, "text": " reproducing properly with machines or with AI, right?", "tokens": [50872, 11408, 2175, 6108, 365, 8379, 420, 365, 7318, 11, 558, 30, 51020], "temperature": 0.0, "avg_logprob": -0.23898110906761813, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.15858766436576843}, {"id": 15, "seek": 4880, "start": 61.92, "end": 64.96, "text": " So the most popular approaches to machine learning today are", "tokens": [51020, 407, 264, 881, 3743, 11587, 281, 3479, 2539, 965, 366, 51172], "temperature": 0.0, "avg_logprob": -0.23898110906761813, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.15858766436576843}, {"id": 16, "seek": 4880, "start": 65.67999999999999, "end": 69.03999999999999, "text": " or Pydheims, I should say, are supervised learning and reinforcement learning.", "tokens": [51208, 420, 9953, 67, 18673, 82, 11, 286, 820, 584, 11, 366, 46533, 2539, 293, 29280, 2539, 13, 51376], "temperature": 0.0, "avg_logprob": -0.23898110906761813, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.15858766436576843}, {"id": 17, "seek": 4880, "start": 70.0, "end": 76.24, "text": " And there is too many efficient supervised learning requires many samples for learning anything.", "tokens": [51424, 400, 456, 307, 886, 867, 7148, 46533, 2539, 7029, 867, 10938, 337, 2539, 1340, 13, 51736], "temperature": 0.0, "avg_logprob": -0.23898110906761813, "compression_ratio": 1.7399103139013452, "no_speech_prob": 0.15858766436576843}, {"id": 18, "seek": 7624, "start": 77.03999999999999, "end": 82.56, "text": " And reinforcement learning requires a ridiculously large number of trial and errors to", "tokens": [50404, 400, 29280, 2539, 7029, 257, 41358, 2416, 1230, 295, 7308, 293, 13603, 281, 50680], "temperature": 0.0, "avg_logprob": -0.15239038272779815, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.059618595987558365}, {"id": 19, "seek": 7624, "start": 82.56, "end": 84.39999999999999, "text": " for a system to learn anything.", "tokens": [50680, 337, 257, 1185, 281, 1466, 1340, 13, 50772], "temperature": 0.0, "avg_logprob": -0.15239038272779815, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.059618595987558365}, {"id": 20, "seek": 7624, "start": 86.56, "end": 88.39999999999999, "text": " And that's why we don't have self-driving cars.", "tokens": [50880, 400, 300, 311, 983, 321, 500, 380, 362, 2698, 12, 47094, 5163, 13, 50972], "temperature": 0.0, "avg_logprob": -0.15239038272779815, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.059618595987558365}, {"id": 21, "seek": 7624, "start": 88.39999999999999, "end": 90.24, "text": " That's a big leap for one to the other.", "tokens": [50972, 663, 311, 257, 955, 19438, 337, 472, 281, 264, 661, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15239038272779815, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.059618595987558365}, {"id": 22, "seek": 7624, "start": 90.24, "end": 95.36, "text": " Okay, so to solve difficult problems, you have to have a lot of", "tokens": [51064, 1033, 11, 370, 281, 5039, 2252, 2740, 11, 291, 362, 281, 362, 257, 688, 295, 51320], "temperature": 0.0, "avg_logprob": -0.15239038272779815, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.059618595987558365}, {"id": 23, "seek": 7624, "start": 96.47999999999999, "end": 99.44, "text": " human annotations for supervised learning to work.", "tokens": [51376, 1952, 25339, 763, 337, 46533, 2539, 281, 589, 13, 51524], "temperature": 0.0, "avg_logprob": -0.15239038272779815, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.059618595987558365}, {"id": 24, "seek": 7624, "start": 99.44, "end": 102.96, "text": " And to solve those difficult problems with reinforcement learning, you have to have", "tokens": [51524, 400, 281, 5039, 729, 2252, 2740, 365, 29280, 2539, 11, 291, 362, 281, 362, 51700], "temperature": 0.0, "avg_logprob": -0.15239038272779815, "compression_ratio": 1.7456896551724137, "no_speech_prob": 0.059618595987558365}, {"id": 25, "seek": 10296, "start": 102.96, "end": 107.67999999999999, "text": " some way to maybe simulate that problem such that you can do that large scale kind of learning", "tokens": [50364, 512, 636, 281, 1310, 27817, 300, 1154, 1270, 300, 291, 393, 360, 300, 2416, 4373, 733, 295, 2539, 50600], "temperature": 0.0, "avg_logprob": -0.13125195754201788, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.007460578344762325}, {"id": 26, "seek": 10296, "start": 107.67999999999999, "end": 109.36, "text": " that reinforcement learning requires.", "tokens": [50600, 300, 29280, 2539, 7029, 13, 50684], "temperature": 0.0, "avg_logprob": -0.13125195754201788, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.007460578344762325}, {"id": 27, "seek": 10296, "start": 109.36, "end": 116.32, "text": " Right, so how is it that most teenagers can learn to drive a car in about 20 hours of practice,", "tokens": [50684, 1779, 11, 370, 577, 307, 309, 300, 881, 23618, 393, 1466, 281, 3332, 257, 1032, 294, 466, 945, 2496, 295, 3124, 11, 51032], "temperature": 0.0, "avg_logprob": -0.13125195754201788, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.007460578344762325}, {"id": 28, "seek": 10296, "start": 117.19999999999999, "end": 124.16, "text": " whereas even with millions of hours of simulated practice, self-driving car can't actually learn", "tokens": [51076, 9735, 754, 365, 6803, 295, 2496, 295, 41713, 3124, 11, 2698, 12, 47094, 1032, 393, 380, 767, 1466, 51424], "temperature": 0.0, "avg_logprob": -0.13125195754201788, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.007460578344762325}, {"id": 29, "seek": 10296, "start": 124.16, "end": 129.76, "text": " to drive itself properly? And so obviously we're missing something, right? And it's quite obvious", "tokens": [51424, 281, 3332, 2564, 6108, 30, 400, 370, 2745, 321, 434, 5361, 746, 11, 558, 30, 400, 309, 311, 1596, 6322, 51704], "temperature": 0.0, "avg_logprob": -0.13125195754201788, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.007460578344762325}, {"id": 30, "seek": 12976, "start": 129.84, "end": 136.16, "text": " for a lot of people that the immediate response you get from people is, well, humans use their", "tokens": [50368, 337, 257, 688, 295, 561, 300, 264, 11629, 4134, 291, 483, 490, 561, 307, 11, 731, 11, 6255, 764, 641, 50684], "temperature": 0.0, "avg_logprob": -0.14709314409193103, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.008151347748935223}, {"id": 31, "seek": 12976, "start": 136.16, "end": 142.79999999999998, "text": " background knowledge to learn faster. And they're right. Now, how was that background knowledge", "tokens": [50684, 3678, 3601, 281, 1466, 4663, 13, 400, 436, 434, 558, 13, 823, 11, 577, 390, 300, 3678, 3601, 51016], "temperature": 0.0, "avg_logprob": -0.14709314409193103, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.008151347748935223}, {"id": 32, "seek": 12976, "start": 142.79999999999998, "end": 149.76, "text": " acquired? And that's the big question. So now you have to ask, how do babies in the first few", "tokens": [51016, 17554, 30, 400, 300, 311, 264, 955, 1168, 13, 407, 586, 291, 362, 281, 1029, 11, 577, 360, 10917, 294, 264, 700, 1326, 51364], "temperature": 0.0, "avg_logprob": -0.14709314409193103, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.008151347748935223}, {"id": 33, "seek": 12976, "start": 149.76, "end": 154.95999999999998, "text": " months of life learn how the world works, mostly by observation, because they can hardly act in the", "tokens": [51364, 2493, 295, 993, 1466, 577, 264, 1002, 1985, 11, 5240, 538, 14816, 11, 570, 436, 393, 13572, 605, 294, 264, 51624], "temperature": 0.0, "avg_logprob": -0.14709314409193103, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.008151347748935223}, {"id": 34, "seek": 15496, "start": 154.96, "end": 159.84, "text": " world. And they learn an enormous amount of background knowledge about the world that may be", "tokens": [50364, 1002, 13, 400, 436, 1466, 364, 11322, 2372, 295, 3678, 3601, 466, 264, 1002, 300, 815, 312, 50608], "temperature": 0.0, "avg_logprob": -0.14492637809665723, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0006631127325817943}, {"id": 35, "seek": 15496, "start": 160.64000000000001, "end": 166.88, "text": " the basis of what we call common sense. This type of learning is not learning a task, it's not", "tokens": [50648, 264, 5143, 295, 437, 321, 818, 2689, 2020, 13, 639, 2010, 295, 2539, 307, 406, 2539, 257, 5633, 11, 309, 311, 406, 50960], "temperature": 0.0, "avg_logprob": -0.14492637809665723, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0006631127325817943}, {"id": 36, "seek": 15496, "start": 167.60000000000002, "end": 172.24, "text": " being reinforced for anything, it's just observing the world and figuring out how it works.", "tokens": [50996, 885, 31365, 337, 1340, 11, 309, 311, 445, 22107, 264, 1002, 293, 15213, 484, 577, 309, 1985, 13, 51228], "temperature": 0.0, "avg_logprob": -0.14492637809665723, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0006631127325817943}, {"id": 37, "seek": 15496, "start": 173.60000000000002, "end": 179.28, "text": " Building world models, learning world models. How do we do this? And how do we reproduce this", "tokens": [51296, 18974, 1002, 5245, 11, 2539, 1002, 5245, 13, 1012, 360, 321, 360, 341, 30, 400, 577, 360, 321, 29501, 341, 51580], "temperature": 0.0, "avg_logprob": -0.14492637809665723, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.0006631127325817943}, {"id": 38, "seek": 17928, "start": 179.36, "end": 186.64000000000001, "text": " in machines? So self-supervised learning is one instance or one attempt at trying to reproduce", "tokens": [50368, 294, 8379, 30, 407, 2698, 12, 48172, 24420, 2539, 307, 472, 5197, 420, 472, 5217, 412, 1382, 281, 29501, 50732], "temperature": 0.0, "avg_logprob": -0.19884762011076274, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.18716514110565186}, {"id": 39, "seek": 17928, "start": 186.64000000000001, "end": 192.16, "text": " this kind of learning. Okay, so you're looking at just observation. So not even the interacting part", "tokens": [50732, 341, 733, 295, 2539, 13, 1033, 11, 370, 291, 434, 1237, 412, 445, 14816, 13, 407, 406, 754, 264, 18017, 644, 51008], "temperature": 0.0, "avg_logprob": -0.19884762011076274, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.18716514110565186}, {"id": 40, "seek": 17928, "start": 192.16, "end": 197.76, "text": " of a child. It's just sitting there watching Mom and Dad walk around, pick up stuff, all of that.", "tokens": [51008, 295, 257, 1440, 13, 467, 311, 445, 3798, 456, 1976, 5576, 293, 5639, 1792, 926, 11, 1888, 493, 1507, 11, 439, 295, 300, 13, 51288], "temperature": 0.0, "avg_logprob": -0.19884762011076274, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.18716514110565186}, {"id": 41, "seek": 17928, "start": 197.76, "end": 202.96, "text": " That's what you mean by background knowledge. Perhaps not even watching Mom and Dad just watching", "tokens": [51288, 663, 311, 437, 291, 914, 538, 3678, 3601, 13, 10517, 406, 754, 1976, 5576, 293, 5639, 445, 1976, 51548], "temperature": 0.0, "avg_logprob": -0.19884762011076274, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.18716514110565186}, {"id": 42, "seek": 17928, "start": 202.96, "end": 208.08, "text": " the world go by. Just having eyes open or having eyes closed or the very active opening and closing", "tokens": [51548, 264, 1002, 352, 538, 13, 1449, 1419, 2575, 1269, 420, 1419, 2575, 5395, 420, 264, 588, 4967, 5193, 293, 10377, 51804], "temperature": 0.0, "avg_logprob": -0.19884762011076274, "compression_ratio": 1.6872852233676976, "no_speech_prob": 0.18716514110565186}, {"id": 43, "seek": 20808, "start": 208.16000000000003, "end": 214.4, "text": " eyes that the world appears and disappears, all that basic information. And you're saying in order to", "tokens": [50368, 2575, 300, 264, 1002, 7038, 293, 25527, 11, 439, 300, 3875, 1589, 13, 400, 291, 434, 1566, 294, 1668, 281, 50680], "temperature": 0.0, "avg_logprob": -0.16950484201417748, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.06484976410865784}, {"id": 44, "seek": 20808, "start": 214.4, "end": 220.08, "text": " learn to drive, like the reason humans are able to learn to drive quickly, some faster than others,", "tokens": [50680, 1466, 281, 3332, 11, 411, 264, 1778, 6255, 366, 1075, 281, 1466, 281, 3332, 2661, 11, 512, 4663, 813, 2357, 11, 50964], "temperature": 0.0, "avg_logprob": -0.16950484201417748, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.06484976410865784}, {"id": 45, "seek": 20808, "start": 220.08, "end": 224.32000000000002, "text": " is because of the background knowledge, they were able to watch cars operate in the world in the", "tokens": [50964, 307, 570, 295, 264, 3678, 3601, 11, 436, 645, 1075, 281, 1159, 5163, 9651, 294, 264, 1002, 294, 264, 51176], "temperature": 0.0, "avg_logprob": -0.16950484201417748, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.06484976410865784}, {"id": 46, "seek": 20808, "start": 224.32000000000002, "end": 228.56, "text": " many years leading up to it, the physics of basic subjects, all that kind of stuff. That's right.", "tokens": [51176, 867, 924, 5775, 493, 281, 309, 11, 264, 10649, 295, 3875, 13066, 11, 439, 300, 733, 295, 1507, 13, 663, 311, 558, 13, 51388], "temperature": 0.0, "avg_logprob": -0.16950484201417748, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.06484976410865784}, {"id": 47, "seek": 20808, "start": 228.56, "end": 232.48000000000002, "text": " I mean the basic physics of objects, you don't even know, you don't even need to know how", "tokens": [51388, 286, 914, 264, 3875, 10649, 295, 6565, 11, 291, 500, 380, 754, 458, 11, 291, 500, 380, 754, 643, 281, 458, 577, 51584], "temperature": 0.0, "avg_logprob": -0.16950484201417748, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.06484976410865784}, {"id": 48, "seek": 20808, "start": 232.48000000000002, "end": 236.96, "text": " car works, because that you can run fairly quickly. I mean the example I use very often is you're", "tokens": [51584, 1032, 1985, 11, 570, 300, 291, 393, 1190, 6457, 2661, 13, 286, 914, 264, 1365, 286, 764, 588, 2049, 307, 291, 434, 51808], "temperature": 0.0, "avg_logprob": -0.16950484201417748, "compression_ratio": 1.8717948717948718, "no_speech_prob": 0.06484976410865784}, {"id": 49, "seek": 23696, "start": 236.96, "end": 243.36, "text": " driving next to a cliff. And you know in advance, because of your understanding of", "tokens": [50364, 4840, 958, 281, 257, 22316, 13, 400, 291, 458, 294, 7295, 11, 570, 295, 428, 3701, 295, 50684], "temperature": 0.0, "avg_logprob": -0.169854736328125, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0026142457500100136}, {"id": 50, "seek": 23696, "start": 243.36, "end": 247.44, "text": " intuitive physics, that if you turn the wheel to the right, the car will be out to the right,", "tokens": [50684, 21769, 10649, 11, 300, 498, 291, 1261, 264, 5589, 281, 264, 558, 11, 264, 1032, 486, 312, 484, 281, 264, 558, 11, 50888], "temperature": 0.0, "avg_logprob": -0.169854736328125, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0026142457500100136}, {"id": 51, "seek": 23696, "start": 247.44, "end": 251.36, "text": " we'll run off the cliff, fall off the cliff, and nothing good will come out of this.", "tokens": [50888, 321, 603, 1190, 766, 264, 22316, 11, 2100, 766, 264, 22316, 11, 293, 1825, 665, 486, 808, 484, 295, 341, 13, 51084], "temperature": 0.0, "avg_logprob": -0.169854736328125, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0026142457500100136}, {"id": 52, "seek": 23696, "start": 251.36, "end": 258.72, "text": " But if you are a sort of tabularized reinforcement learning system that doesn't have a model of", "tokens": [51084, 583, 498, 291, 366, 257, 1333, 295, 4421, 1040, 1602, 29280, 2539, 1185, 300, 1177, 380, 362, 257, 2316, 295, 51452], "temperature": 0.0, "avg_logprob": -0.169854736328125, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0026142457500100136}, {"id": 53, "seek": 23696, "start": 258.72, "end": 264.48, "text": " the world, you have to repeat folding off this cliff thousands of times before you figure out", "tokens": [51452, 264, 1002, 11, 291, 362, 281, 7149, 25335, 766, 341, 22316, 5383, 295, 1413, 949, 291, 2573, 484, 51740], "temperature": 0.0, "avg_logprob": -0.169854736328125, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0026142457500100136}, {"id": 54, "seek": 26448, "start": 264.48, "end": 268.88, "text": " it's a bad idea. And then a few more thousand times before you figure out how to not do it.", "tokens": [50364, 309, 311, 257, 1578, 1558, 13, 400, 550, 257, 1326, 544, 4714, 1413, 949, 291, 2573, 484, 577, 281, 406, 360, 309, 13, 50584], "temperature": 0.0, "avg_logprob": -0.1325702929715498, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.03037310019135475}, {"id": 55, "seek": 26448, "start": 269.44, "end": 273.04, "text": " And then a few more million times before you figure out how to not do it in every situation", "tokens": [50612, 400, 550, 257, 1326, 544, 2459, 1413, 949, 291, 2573, 484, 577, 281, 406, 360, 309, 294, 633, 2590, 50792], "temperature": 0.0, "avg_logprob": -0.1325702929715498, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.03037310019135475}, {"id": 56, "seek": 26448, "start": 273.04, "end": 278.72, "text": " you ever encounter. So self-supervised learning still has to have some source of truth", "tokens": [50792, 291, 1562, 8593, 13, 407, 2698, 12, 48172, 24420, 2539, 920, 575, 281, 362, 512, 4009, 295, 3494, 51076], "temperature": 0.0, "avg_logprob": -0.1325702929715498, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.03037310019135475}, {"id": 57, "seek": 26448, "start": 279.44, "end": 285.76, "text": " being told to it by somebody, by some. And so you have to figure out a way without human", "tokens": [51112, 885, 1907, 281, 309, 538, 2618, 11, 538, 512, 13, 400, 370, 291, 362, 281, 2573, 484, 257, 636, 1553, 1952, 51428], "temperature": 0.0, "avg_logprob": -0.1325702929715498, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.03037310019135475}, {"id": 58, "seek": 26448, "start": 285.76, "end": 290.72, "text": " assistance or without significant amount of human assistance to get that truth from the world.", "tokens": [51428, 9683, 420, 1553, 4776, 2372, 295, 1952, 9683, 281, 483, 300, 3494, 490, 264, 1002, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1325702929715498, "compression_ratio": 1.9156118143459915, "no_speech_prob": 0.03037310019135475}, {"id": 59, "seek": 29072, "start": 290.72, "end": 292.72, "text": " So the", "tokens": [50364, 407, 264, 50464], "temperature": 0.0, "avg_logprob": -0.9150799751281739, "compression_ratio": 0.42857142857142855, "no_speech_prob": 0.34985747933387756}], "language": "en"}