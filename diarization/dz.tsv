start	end	text
0	4720	The following is a conversation with Jan Lekun, his second time on the podcast.
4720	9120	He is the chief AI scientist at Metta, formerly Facebook,
9120	15600	professor at NYU, touring award winner, one of the seminal figures in the history
15600	21920	of machine learning and artificial intelligence, and someone who is brilliant and opinionated
21920	25360	in the best kind of way, and so is always fun to talk to.
25840	28560	This is Alex Friedman podcast to support it.
28560	30720	Please check out our sponsors in the description.
30720	33600	And now here's my conversation with Jan Lekun.
33600	38320	You co-wrote the article, self-supervised learning, the dark matter of intelligence.
38320	39520	Great title, by the way.
39520	41120	Will he shine, Ezra?
41120	46960	So let me ask, what is self-supervised learning and why is it the dark matter of intelligence?
46960	48800	I'll start by the dark matter part.
49760	58960	There is obviously a kind of learning that humans and animals are doing that we currently are not
58960	61920	reproducing properly with machines or with AI, right?
61920	64960	So the most popular approaches to machine learning today are
65680	69040	or Pydheims, I should say, are supervised learning and reinforcement learning.
70000	76240	And there is too many efficient supervised learning requires many samples for learning anything.
77040	82560	And reinforcement learning requires a ridiculously large number of trial and errors to
82560	84400	for a system to learn anything.
86560	88400	And that's why we don't have self-driving cars.
88400	90240	That's a big leap for one to the other.
90240	95360	Okay, so to solve difficult problems, you have to have a lot of
96480	99440	human annotations for supervised learning to work.
99440	102960	And to solve those difficult problems with reinforcement learning, you have to have
102960	107680	some way to maybe simulate that problem such that you can do that large scale kind of learning
107680	109360	that reinforcement learning requires.
109360	116320	Right, so how is it that most teenagers can learn to drive a car in about 20 hours of practice,
117200	124160	whereas even with millions of hours of simulated practice, self-driving car can't actually learn
124160	129760	to drive itself properly? And so obviously we're missing something, right? And it's quite obvious
129840	136160	for a lot of people that the immediate response you get from people is, well, humans use their
136160	142800	background knowledge to learn faster. And they're right. Now, how was that background knowledge
142800	149760	acquired? And that's the big question. So now you have to ask, how do babies in the first few
149760	154960	months of life learn how the world works, mostly by observation, because they can hardly act in the
154960	159840	world. And they learn an enormous amount of background knowledge about the world that may be
160640	166880	the basis of what we call common sense. This type of learning is not learning a task, it's not
167600	172240	being reinforced for anything, it's just observing the world and figuring out how it works.
173600	179280	Building world models, learning world models. How do we do this? And how do we reproduce this
179360	186640	in machines? So self-supervised learning is one instance or one attempt at trying to reproduce
186640	192160	this kind of learning. Okay, so you're looking at just observation. So not even the interacting part
192160	197760	of a child. It's just sitting there watching Mom and Dad walk around, pick up stuff, all of that.
197760	202960	That's what you mean by background knowledge. Perhaps not even watching Mom and Dad just watching
202960	208080	the world go by. Just having eyes open or having eyes closed or the very active opening and closing
208160	214400	eyes that the world appears and disappears, all that basic information. And you're saying in order to
214400	220080	learn to drive, like the reason humans are able to learn to drive quickly, some faster than others,
220080	224320	is because of the background knowledge, they were able to watch cars operate in the world in the
224320	228560	many years leading up to it, the physics of basic subjects, all that kind of stuff. That's right.
228560	232480	I mean the basic physics of objects, you don't even know, you don't even need to know how
232480	236960	car works, because that you can run fairly quickly. I mean the example I use very often is you're
236960	243360	driving next to a cliff. And you know in advance, because of your understanding of
243360	247440	intuitive physics, that if you turn the wheel to the right, the car will be out to the right,
247440	251360	we'll run off the cliff, fall off the cliff, and nothing good will come out of this.
251360	258720	But if you are a sort of tabularized reinforcement learning system that doesn't have a model of
258720	264480	the world, you have to repeat folding off this cliff thousands of times before you figure out
264480	268880	it's a bad idea. And then a few more thousand times before you figure out how to not do it.
269440	273040	And then a few more million times before you figure out how to not do it in every situation
273040	278720	you ever encounter. So self-supervised learning still has to have some source of truth
279440	285760	being told to it by somebody, by some. And so you have to figure out a way without human
285760	290720	assistance or without significant amount of human assistance to get that truth from the world.
290720	292720	So the
